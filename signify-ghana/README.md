# ğŸ‡¬ğŸ‡­ Signify Ghana - GSL Recognition Platform

A browser-based Ghanaian Sign Language (GSL) recognition and learning platform using MediaPipe Hands and TensorFlow.js.

## ğŸ¯ Features

- **Sign-to-Text + TTS**: Real-time GSL hand sign detection with text output and text-to-speech
- **Text-to-Sign**: Search and watch prerecorded GSL sign videos
- **Learning Hub**: Browse sign video gallery categorized by alphabet, numbers, and words

## ğŸ—ï¸ Project Structure

```
signify-ghana/
â”œâ”€â”€ web/                          # Frontend application
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â”‚   â”œâ”€â”€ index.jsx         # Landing page
â”‚   â”‚   â”‚   â”œâ”€â”€ sign.jsx          # Sign detection page
â”‚   â”‚   â”‚   â””â”€â”€ learn.jsx         # Learning hub
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ WebcamDetector.jsx # MediaPipe + TF.js inference
â”‚   â”‚   â”‚   â””â”€â”€ VideoPlayer.jsx    # Text-to-video module
â”‚   â”‚   â””â”€â”€ index.css             # Global styles
â”‚   â”œâ”€â”€ public/
â”‚   â”‚   â”œâ”€â”€ assets/
â”‚   â”‚   â”‚   â”œâ”€â”€ web_model/        # TF.js model (after training)
â”‚   â”‚   â”‚   â””â”€â”€ videos/           # Sign demonstration videos
â”‚   â”‚   â””â”€â”€ labels.json           # Class labels (after training)
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ vite.config.js
â”œâ”€â”€ model_training/               # ML training pipeline
â”‚   â”œâ”€â”€ collect/
â”‚   â”‚   â”œâ”€â”€ data_collection.html  # Data capture UI
â”‚   â”‚   â””â”€â”€ server.py             # Flask upload endpoint
â”‚   â”œâ”€â”€ train_model.py            # Model training script
â”‚   â”œâ”€â”€ export_tfjs.sh            # TF.js export script
â”‚   â””â”€â”€ samples/                  # Training data (*.jsonl)
â””â”€â”€ README.md
```

## ğŸš€ Quick Start

### Step 1: Collect Training Data

```bash
cd model_training/collect

# Install Python dependencies
pip install flask

# Start the data collection server
python server.py

# Open http://localhost:5000 in your browser
```

**Data Collection Instructions:**

1. Enter a sign label (e.g., "A", "B", "Hello", "Thank you")
2. Click "Start Camera"
3. Position your hand clearly in the frame
4. Click "Capture Sample" repeatedly (30-50 samples per sign)
5. Repeat for all signs you want to recognize

### Step 2: Train the Model

```bash
cd model_training

# Install ML dependencies
pip install tensorflow tensorflowjs scikit-learn numpy

# Train the model (outputs to web/public/)
python train_model.py
```

This will:

- Load samples from `samples/*.jsonl`
- Train a neural network classifier
- Export TF.js model to `../web/public/assets/web_model/`
- Save `labels.json` to `../web/public/`

### Step 3: Add Sign Videos (Optional)

Place demonstration videos in `web/public/assets/videos/`:

- Name format: `{label}.mp4` (lowercase)
- Example: `hello.mp4`, `a.mp4`, `thank_you.mp4`

### Step 4: Run the Web App

```bash
cd web

# Install dependencies
npm install

# Start development server
npm run dev
```

Open http://localhost:5173 in your browser.

## ğŸ“Š Model Specifications

### Input

- **Format**: Float32[63] normalized hand landmark vector
- **Structure**: 21 keypoints Ã— 3 coordinates (x, y, z)
- **Source**: MediaPipe Hands

### Architecture

```
Input (63)
  â†’ Dense(128, ReLU) â†’ Dropout(0.3)
  â†’ Dense(64, ReLU) â†’ Dropout(0.3)
  â†’ Dense(N, Softmax)
```

### Output

- **Format**: Softmax probabilities over N classes
- **Prediction**: argmax â†’ class index â†’ label from `labels.json`

### Data Format (JSONL)

```json
{"landmarks": [x1,y1,z1, x2,y2,z2, ..., x21,y21,z21]}
```

## ğŸ¯ Performance Expectations

- **Prototype level**: Reliable for 8â€“30 classes
- **Recommended samples**: 30-50 per class
- **Confidence threshold**: 0.6 (60%)

## ğŸ”§ Tech Stack

| Component      | Technology       |
| -------------- | ---------------- |
| Hand Detection | MediaPipe Hands  |
| Model Training | TensorFlow/Keras |
| Model Format   | TensorFlow.js    |
| Frontend       | React + Vite     |
| Text-to-Speech | Web Speech API   |
| Styling        | Vanilla CSS      |

## ğŸ› ï¸ Development

### Building for Production

```bash
cd web
npm run build
npm run preview
```

### Project Commands

```bash
# Data collection
cd model_training/collect && python server.py

# Model training
cd model_training && python train_model.py

# Web development
cd web && npm run dev

# Web production build
cd web && npm run build
```

## ğŸ“ Adding New Signs

1. **Collect data**: Use data collection tool to capture 30-50 samples
2. **Retrain model**: Run `python train_model.py`
3. **Add video**: Place `{sign}.mp4` in `web/public/assets/videos/`
4. **Restart app**: Refresh the browser

## ğŸ› Troubleshooting

### Camera not working

- âœ… Grant camera permissions in browser
- âœ… Use HTTPS or localhost
- âœ… Check browser console for errors

### Model not loading

- âœ… Verify `web/public/assets/web_model/model.json` exists
- âœ… Check `web/public/labels.json` exists
- âœ… Check browser network tab for 404 errors

### Low accuracy

- âœ… Collect more samples (50+ per sign recommended)
- âœ… Ensure consistent hand positioning during collection
- âœ… Improve lighting conditions
- âœ… Increase model complexity in `train_model.py`

### Videos not playing

- âœ… Verify video files are in `web/public/assets/videos/`
- âœ… Check filename matches label (lowercase)
- âœ… Ensure video format is MP4

## ğŸ“„ License

This project is built for the UNICEF Startup Lab GSL Hackathon.

## ğŸ¤ Contributing

1. Collect diverse training data
2. Test with different lighting conditions
3. Report issues and bugs
4. Improve model architecture
5. Add more sign categories

## ğŸ“§ Support

For questions and support, please open an issue in the repository.

---

**Built with â¤ï¸ for the Ghanaian Deaf Community**
